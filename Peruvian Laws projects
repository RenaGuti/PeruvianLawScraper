#1.1 Importamos las librerias
#Selenium
from selenium import webdriver
from selenium.webdriver.support.ui import WebDriverWait as wait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.common.keys import Keys
#Otros
import time
import pandas as pd
import threading
import datetime
import os
from xlsxwriter import Workbook
import xlrd
import pandas as pd
import numpy as np
import re
from bs4 import BeautifulSoup

#1.2 funciones a utilizar    
def insert_data(law,row):
    #Leyendo el excel anterior
    df = pd.read_excel(EXCEL_NOMBRE, engine='openpyxl')
    df = df.fillna('')
    sheets = df.to_numpy()
    #Creamos el nuevo excel
    workbook = Workbook(os.path.join(os.getcwd(), EXCEL_NOMBRE))
    worksheet = workbook.add_worksheet()
    #Ingresamos los valores anteriores
    for sheet in sheets:
        for rowA in range(len(sheets)):
            for colA in range(len(sheets[0])):
                worksheet.write(rowA+1, colA, sheets[rowA][colA])
    if(row==1):
        #Ponemos los titulos
        columns=['codigo', 'periodo', 'legislatura', 'fecha_P', 'proponente', 'titulo', 'sumilla', 'observaciones', 'autores', 'coautores', 'adherentes', 'grupo_parlamentario', 'comisiones', 'ult_estado']
        for i in range(len(columns)):
            worksheet.write(0, i, columns[i])
    #Ponemos los datos
    for col, var in enumerate(law, start = 0):
        worksheet.write(row, col, var)
    workbook.close()

#La pagina con todos los proyectos de ley
f1 ="https://www2.congreso.gob.pe/Sicr/TraDocEstProc/CLProLey2016.nsf/Local%20Por%20Numero%20Inverso?OpenView&Start=1&Count=1000"
f2 ="https://www2.congreso.gob.pe/Sicr/TraDocEstProc/CLProLey2016.nsf/Local%20Por%20Numero%20Inverso?OpenView&Start=1001&Count=1000"
f3 ="https://www2.congreso.gob.pe/Sicr/TraDocEstProc/CLProLey2016.nsf/Local%20Por%20Numero%20Inverso?OpenView&Start=2001&Count=1000"
f4 ="https://www2.congreso.gob.pe/Sicr/TraDocEstProc/CLProLey2016.nsf/Local%20Por%20Numero%20Inverso?OpenView&Start=3001&Count=1000"
f5 ="https://www2.congreso.gob.pe/Sicr/TraDocEstProc/CLProLey2016.nsf/Local%20Por%20Numero%20Inverso?OpenView&Start=4001&Count=1000"
f6 ="https://www2.congreso.gob.pe/Sicr/TraDocEstProc/CLProLey2016.nsf/Local%20Por%20Numero%20Inverso?OpenView&Start=5001&Count=1000"
f7 ="https://www2.congreso.gob.pe/Sicr/TraDocEstProc/CLProLey2016.nsf/Local%20Por%20Numero%20Inverso?OpenView&Start=6001&Count=1000"
f8 ="https://www2.congreso.gob.pe/Sicr/TraDocEstProc/CLProLey2016.nsf/Local%20Por%20Numero%20Inverso?OpenView&Start=7001&Count=1000"
f9 ="https://www2.congreso.gob.pe/Sicr/TraDocEstProc/CLProLey2016.nsf/Local%20Por%20Numero%20Inverso?OpenView&Start=8001&Count=1000"

fuentes=[f1,f2,f3,f4,f5,f6,f7,f8,f9]

#Abrimos la pagina
ruta_driver='chromedriver'
driver = webdriver.Chrome(executable_path=ruta_driver)
driver.set_page_load_timeout(60)
driver.maximize_window()

#Extraemos todos los links y status
all_tr=[]
for fuente in fuentes:
    driver.get(fuente)
    print("Searching in "+fuente)
    driver.implicitly_wait(1000)
    time.sleep(2)
    soup = BeautifulSoup(driver.page_source, 'lxml')
    all_tr = all_tr+soup.find_all('tr')[3:-2]
    print("len:"+str(len(all_tr)))

#Procesamos los links
links=[]
status=[]
for tr in all_tr:
    all_td=tr.find_all('td')
    try:
        links.append("https://www2.congreso.gob.pe"+all_td[0].find('a')["href"][:-13])
    except:
        continue
    status.append(all_td[4].get_text())

#Proceso directo


periodo='//table[not(@id) or not(@class)]/tbody/tr[2]/td/table/tbody/tr[1]/td[2]'
legislatura='//table[not(@id) or not(@class)]/tbody/tr[2]/td/table/tbody/tr[2]/td[2]'
fecha_P='//table[not(@id) or not(@class)]/tbody/tr[2]/td/table/tbody/tr[3]/td[2]'
proponente='//table[not(@id) or not(@class)]/tbody/tr[2]/td/table/tbody/tr[4]/td[2]'
titulo = '//table[not(@id) or not(@class)]/tbody/tr[2]/td/table/tbody/tr[6]/td[1]/font'
sumilla = '//table[not(@id) or not(@class)]/tbody/tr[2]/td/table/tbody/tr[7]/td[1]/font'
#Falta procesar
codigo='//table[not(@id) or not(@class)]/tbody/tr'
autores = '//table[not(@id) or not(@class)]/tbody/tr[2]/td/table/tbody/tr[5]/td[1]'

xpaths = [codigo, periodo, legislatura, fecha_P, proponente, titulo, sumilla, autores]
#xpaths = [codigo, periodo, legislatura, fecha_P, proponente, titulo, sumilla, autores, comisiones]
#[codigo, periodo, legislatura, fecha_P, proponente, titulo, sumilla, observaciones, autores, coautores, adherentes, grupo_parlamentario, comisiones, ult_estado]

#3.0 Configuramos parametros iniciales
EXCEL_NOMBRE="1001-1200.xlsx"
ruta_driver='chromedriver.exe'
driver = webdriver.Chrome(executable_path=ruta_driver)
driver.set_page_load_timeout(60)
driver.maximize_window()
if not EXCEL_NOMBRE in os.listdir():
    workbook = Workbook(os.path.join(os.getcwd(), EXCEL_NOMBRE))
    worksheet = workbook.add_worksheet()
    workbook.close()

#Acá ponen a partir de que linea quieren que se agregen las nuevas instancias
row=1

for l,s in zip(links[1000:1201], status[1000:1201]):
    data=[]
    driver.get(l)
    #Extremos la información con xpath
    print("Inserting Row "+str(row)+" from "+l)
    driver.implicitly_wait(1000)
    time.sleep(2)
    for i in xpaths: 
        data.append(driver.find_element(by=By.XPATH, value=i).text)
    #Terminamos de procesar algunas variables
    data[0]=data[0].replace('Expediente del "','').replace(' "','').replace('','')
    data.insert(7, '')
    try:
        data.insert(9, data[8].split("\n")[0][21:])
        data[8]=data[8].split("\n")[1]
    except:
        data.insert(9,"")
        data[8]=""
    data.insert(9, '')
    soup = BeautifulSoup(driver.page_source, 'lxml')
    all_tr = soup.find_all('tr')
    if all_tr[9].get_text()[0:16]=='Envío a Comisión':
        comisiones = '//table[not(@id) or not(@class)]/tbody/tr[2]/td/table/tbody/tr[8]/td[1]/font'
        data.insert(10,"")
    else:
        adherentes = '//table[not(@id) or not(@class)]/tbody/tr[2]/td/table/tbody/tr[8]/td[2]'
        comisiones = '//table[not(@id) or not(@class)]/tbody/tr[2]/td/table/tbody/tr[9]/td[1]/font'
        data.insert(10, driver.find_element(by=By.XPATH, value=adherentes).text)
    data.append(driver.find_element(by=By.XPATH, value=comisiones).text)
    obs=re.sub("// ","",re.sub("\d+", "", data[-1])).split("\n")
    if obs[0]!='':
        for ob in obs:
            if ob[:11]=='En comisión':
                data[-1]=ob[12:]
    data.append(s)
    #Insertamos la información en el excel
    insert_data(data,row)
    row+=1
################################################################################################################
#3.0 Configuramos parametros iniciales
EXCEL_NOMBRE="1201-1400.xlsx"
ruta_driver='chromedriver.exe'
driver = webdriver.Chrome(executable_path=ruta_driver)
driver.set_page_load_timeout(60)
driver.maximize_window()
if not EXCEL_NOMBRE in os.listdir():
    workbook = Workbook(os.path.join(os.getcwd(), EXCEL_NOMBRE))
    worksheet = workbook.add_worksheet()
    workbook.close()

#Acá ponen a partir de que linea quieren que se agregen las nuevas instancias
row=1
#Aca cambian los ordenes:
#Jose Luciano [:2000]
#Mire [2001:4000]
#Merry [4001:6000]
#Joso Fabrozo [6001:]
for l,s in zip(links[1200:1401], status[1200:1401]):
    data=[]
    driver.get(l)
    #Extremos la información con xpath
    print("Inserting Row "+str(row)+" from "+l)
    driver.implicitly_wait(1000)
    time.sleep(2)
    for i in xpaths:
        data.append(driver.find_element(by=By.XPATH, value=i).text)
    #Terminamos de procesar algunas variables
    data[0]=data[0].replace('Expediente del "','').replace(' "','').replace('','')
    data.insert(7, '')
    try:
        data.insert(9, data[8].split("\n")[0][21:])
        data[8]=data[8].split("\n")[1]
    except:
        data.insert(9,"")
        data[8]=""
    data.insert(9, '')
    soup = BeautifulSoup(driver.page_source, 'lxml')
    all_tr = soup.find_all('tr')
    if all_tr[9].get_text()[0:16]=='Envío a Comisión':
        comisiones = '//table[not(@id) or not(@class)]/tbody/tr[2]/td/table/tbody/tr[8]/td[1]/font'
        data.insert(10,"")
    else:
        adherentes = '//table[not(@id) or not(@class)]/tbody/tr[2]/td/table/tbody/tr[8]/td[2]'
        comisiones = '//table[not(@id) or not(@class)]/tbody/tr[2]/td/table/tbody/tr[9]/td[1]/font'
        data.insert(10, driver.find_element(by=By.XPATH, value=adherentes).text)
    data.append(driver.find_element(by=By.XPATH, value=comisiones).text)
    obs=re.sub("// ","",re.sub("\d+", "", data[-1])).split("\n")
    if obs[0]!='':
        for ob in obs:
            if ob[:11]=='En comisión':
                data[-1]=ob[12:]
    data.append(s)
    #Insertamos la información en el excel
    insert_data(data,row)
    row+=1
################################################################################################################
#3.0 Configuramos parametros iniciales
EXCEL_NOMBRE="1401-1600.xlsx"
ruta_driver='chromedriver.exe'
driver = webdriver.Chrome(executable_path=ruta_driver)
driver.set_page_load_timeout(60)
driver.maximize_window()
if not EXCEL_NOMBRE in os.listdir():
    workbook = Workbook(os.path.join(os.getcwd(), EXCEL_NOMBRE))
    worksheet = workbook.add_worksheet()
    workbook.close()

#Acá ponen a partir de que linea quieren que se agregen las nuevas instancias
row=1
#Aca cambian los ordenes:
#Jose Luciano [:2000]
#Mire [2001:4000]
#Merry [4001:6000]
#Joso Fabrozo [6001:]
for l,s in zip(links[1400:1601], status[1400:1601]):
    data=[]
    driver.get(l)
    #Extremos la información con xpath
    print("Inserting Row "+str(row)+" from "+l)
    driver.implicitly_wait(1000)
    time.sleep(2)
    for i in xpaths:
        data.append(driver.find_element(by=By.XPATH, value=i).text)
    #Terminamos de procesar algunas variables
    data[0]=data[0].replace('Expediente del "','').replace(' "','').replace('','')
    data.insert(7, '')
    try:
        data.insert(9, data[8].split("\n")[0][21:])
        data[8]=data[8].split("\n")[1]
    except:
        data.insert(9,"")
        data[8]=""
    data.insert(9, '')
    soup = BeautifulSoup(driver.page_source, 'lxml')
    all_tr = soup.find_all('tr')
    if all_tr[9].get_text()[0:16]=='Envío a Comisión':
        comisiones = '//table[not(@id) or not(@class)]/tbody/tr[2]/td/table/tbody/tr[8]/td[1]/font'
        data.insert(10,"")
    else:
        adherentes = '//table[not(@id) or not(@class)]/tbody/tr[2]/td/table/tbody/tr[8]/td[2]'
        comisiones = '//table[not(@id) or not(@class)]/tbody/tr[2]/td/table/tbody/tr[9]/td[1]/font'
        data.insert(10, driver.find_element(by=By.XPATH, value=adherentes).text)
    data.append(driver.find_element(by=By.XPATH, value=comisiones).text)
    obs=re.sub("// ","",re.sub("\d+", "", data[-1])).split("\n")
    if obs[0]!='':
        for ob in obs:
            if ob[:11]=='En comisión':
                data[-1]=ob[12:]
    data.append(s)
    #Insertamos la información en el excel
    insert_data(data,row)
    row+=1
################################################################################################################
#3.0 Configuramos parametros iniciales
EXCEL_NOMBRE="1601-1800.xlsx"
ruta_driver='chromedriver.exe'
driver = webdriver.Chrome(executable_path=ruta_driver)
driver.set_page_load_timeout(60)
driver.maximize_window()
if not EXCEL_NOMBRE in os.listdir():
    workbook = Workbook(os.path.join(os.getcwd(), EXCEL_NOMBRE))
    worksheet = workbook.add_worksheet()
    workbook.close()

#Acá ponen a partir de que linea quieren que se agregen las nuevas instancias
row=1
#Aca cambian los ordenes:
#Jose Luciano [:2000]
#Mire [2001:4000]
#Merry [4001:6000]
#Joso Fabrozo [6001:]
for l,s in zip(links[1600:1801], status[1600:1801]):
    data=[]
    driver.get(l)
    #Extremos la información con xpath
    print("Inserting Row "+str(row)+" from "+l)
    driver.implicitly_wait(1000)
    time.sleep(2)
    for i in xpaths:
        data.append(driver.find_element(by=By.XPATH, value=i).text)
    #Terminamos de procesar algunas variables
    data[0]=data[0].replace('Expediente del "','').replace(' "','').replace('','')
    data.insert(7, '')
    try:
        data.insert(9, data[8].split("\n")[0][21:])
        data[8]=data[8].split("\n")[1]
    except:
        data.insert(9,"")
        data[8]=""
    data.insert(9, '')
    soup = BeautifulSoup(driver.page_source, 'lxml')
    all_tr = soup.find_all('tr')
    if all_tr[9].get_text()[0:16]=='Envío a Comisión':
        comisiones = '//table[not(@id) or not(@class)]/tbody/tr[2]/td/table/tbody/tr[8]/td[1]/font'
        data.insert(10,"")
    else:
        adherentes = '//table[not(@id) or not(@class)]/tbody/tr[2]/td/table/tbody/tr[8]/td[2]'
        comisiones = '//table[not(@id) or not(@class)]/tbody/tr[2]/td/table/tbody/tr[9]/td[1]/font'
        data.insert(10, driver.find_element(by=By.XPATH, value=adherentes).text)
    data.append(driver.find_element(by=By.XPATH, value=comisiones).text)
    obs=re.sub("// ","",re.sub("\d+", "", data[-1])).split("\n")
    if obs[0]!='':
        for ob in obs:
            if ob[:11]=='En comisión':
                data[-1]=ob[12:]
    data.append(s)
    #Insertamos la información en el excel
    insert_data(data,row)
    row+=1

################################################################################################################
#3.0 Configuramos parametros iniciales
EXCEL_NOMBRE="1801-2000.xlsx"
ruta_driver='chromedriver.exe'
driver = webdriver.Chrome(executable_path=ruta_driver)
driver.set_page_load_timeout(60)
driver.maximize_window()
if not EXCEL_NOMBRE in os.listdir():
    workbook = Workbook(os.path.join(os.getcwd(), EXCEL_NOMBRE))
    worksheet = workbook.add_worksheet()
    workbook.close()

#Acá ponen a partir de que linea quieren que se agregen las nuevas instancias
row=1
#Aca cambian los ordenes:
#Jose Luciano [:2000]
#Mire [2001:4000]
#Merry [4001:6000]
#Joso Fabrozo [6001:]
for l,s in zip(links[1800:2001], status[1800:2001]):
    data=[]
    driver.get(l)
    #Extremos la información con xpath
    print("Inserting Row "+str(row)+" from "+l)
    driver.implicitly_wait(1000)
    time.sleep(2)
    for i in xpaths:
        data.append(driver.find_element(by=By.XPATH, value=i).text)
    #Terminamos de procesar algunas variables
    data[0]=data[0].replace('Expediente del "','').replace(' "','').replace('','')
    data.insert(7, '')
    try:
        data.insert(9, data[8].split("\n")[0][21:])
        data[8]=data[8].split("\n")[1]
    except:
        data.insert(9,"")
        data[8]=""
    data.insert(9, '')
    soup = BeautifulSoup(driver.page_source, 'lxml')
    all_tr = soup.find_all('tr')
    if all_tr[9].get_text()[0:16]=='Envío a Comisión':
        comisiones = '//table[not(@id) or not(@class)]/tbody/tr[2]/td/table/tbody/tr[8]/td[1]/font'
        data.insert(10,"")
    else:
        adherentes = '//table[not(@id) or not(@class)]/tbody/tr[2]/td/table/tbody/tr[8]/td[2]'
        comisiones = '//table[not(@id) or not(@class)]/tbody/tr[2]/td/table/tbody/tr[9]/td[1]/font'
        data.insert(10, driver.find_element(by=By.XPATH, value=adherentes).text)
    data.append(driver.find_element(by=By.XPATH, value=comisiones).text)
    obs=re.sub("// ","",re.sub("\d+", "", data[-1])).split("\n")
    if obs[0]!='':
        for ob in obs:
            if ob[:11]=='En comisión':
                data[-1]=ob[12:]
    data.append(s)
    #Insertamos la información en el excel
    insert_data(data,row)
    row+=1
